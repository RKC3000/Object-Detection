{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOwhFB/gs7x2pUgaHbWpWuC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RKC3000/Object-Detection/blob/main/3_Day_Object_Detection_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Object Detection Challenge**\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "izua9Zc-qyPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**YOLOv5** setup"
      ],
      "metadata": {
        "id": "g88iIYptJlrh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/yolov5  # clone\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt  # install\n",
        "\n",
        "import torch\n",
        "import utils\n",
        "display = utils.notebook_init()  # checks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YklsrC6GJOoV",
        "outputId": "19892307-a66f-4bb1-b96d-016a4e53bbab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "YOLOv5 ðŸš€ v7.0-140-g1db9533 Python-3.9.16 torch-2.0.0+cu118 CPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 23.4/107.7 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download Dataset**"
      ],
      "metadata": {
        "id": "w4hnoM2vM-jY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash data/scripts/get_coco.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-Icmzg3J7Rc",
        "outputId": "6bac618a-843e-4a57-919b-4fb5bd81f3c7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017labels.zip  ...\n",
            "Downloading http://images.cocodataset.org/zips/train2017.zip ...\n",
            "Downloading http://images.cocodataset.org/zips/val2017.zip ...\n",
            "######################################################################## 100.0%\n",
            "######################################################################## 100.0%\n",
            "######################################################################## 100.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.utils import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "\n",
        "class obj_dec(Dataset):\n",
        "  def __init__(self,image_folder,annotations_file,transform = None):\n",
        "    self.image_folder = image_folder\n",
        "    self.annotations = pd.read_csv(annotations_file)\n",
        "    self.transform = transform\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.annotations)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    # LoadÂ theÂ imageÂ andÂ convertÂ toÂ RGB\n",
        "    image_path = os.path.join(self.image_folder, self.annotations.iloc[idx,0])\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "    # Get the annotations for this image\n",
        "    annotations = self.annotations.iloc[idx,1:]\n",
        "\n",
        "    # Convert annotations to tensor\n",
        "    annotations = torch.tensor(annotations.astype('float32'))\n",
        "\n",
        "    # Apply transform if available\n",
        "    if self.transform:\n",
        "      image = self.transform(image)\n",
        "\n",
        "      return image, annotations\n",
        "\n",
        "  transform = transforms.Compose([\n",
        "      transforms.Resize((224,224)),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "  ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "RSpllxLmO5YE",
        "outputId": "480af16c-53ca-4a8b-ba58-9bda6c6367cb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-95f48f7638ce>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mread_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mobj_dec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage_folder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mannotations_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_folder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imagenet_data = torchvision.datasets.ImageNet('/content/datasets/coco128/images/')\n",
        "data_loader = torch.utils.data.DataLoader(imagenet_data,\n",
        "                                          batch_size=4,\n",
        "                                          shuffle=True,\n",
        "                                          num_workers=args.nThreads)"
      ],
      "metadata": {
        "id": "lPwdye_qKUdQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}